{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:21.318591Z",
     "iopub.status.busy": "2025-08-28T08:27:21.318305Z",
     "iopub.status.idle": "2025-08-28T08:27:27.580233Z",
     "shell.execute_reply": "2025-08-28T08:27:27.578780Z",
     "shell.execute_reply.started": "2025-08-28T08:27:21.318555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "import json\n",
    "import copy\n",
    "from IPython.display import Markdown\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from mrmr import mrmr_classif\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.0. Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:40.064196Z",
     "iopub.status.busy": "2025-08-28T08:27:40.063883Z",
     "iopub.status.idle": "2025-08-28T08:27:40.078454Z",
     "shell.execute_reply": "2025-08-28T08:27:40.077290Z",
     "shell.execute_reply.started": "2025-08-28T08:27:40.064165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "rseed = 42\n",
    "set_seed(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:40.080619Z",
     "iopub.status.busy": "2025-08-28T08:27:40.080198Z",
     "iopub.status.idle": "2025-08-28T08:27:40.089595Z",
     "shell.execute_reply": "2025-08-28T08:27:40.088468Z",
     "shell.execute_reply.started": "2025-08-28T08:27:40.080574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'tcga-gbm-methxgexcnv-2000-3-omics'\n",
    "COHORT = 'TCGA_GBM_METHxGExCNV_2000x2000x2000_MinMaxScaler'\n",
    "\n",
    "postfix_tr = '_tr'\n",
    "postfix_te = '_val'\n",
    "\n",
    "data_folder = f'/kaggle/input/{dataset_name}/{COHORT}'\n",
    "model_folder = '/kaggle/working/models'\n",
    "\n",
    "num_models = 4\n",
    "loc_file_json_id_omic = data_folder + '/1/dct_index_subtype.json'\n",
    "with open(loc_file_json_id_omic) as file_json_id_omic:\n",
    "    dct_LABEL_MAPPING_NAME = json.load(file_json_id_omic)\n",
    "    # dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()} # convert str number key to int\n",
    "LABEL_MAPPING_NAME = dct_LABEL_MAPPING_NAME.values()\n",
    "\n",
    "num_subtypes = len(LABEL_MAPPING_NAME)\n",
    "view_list = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:40.135389Z",
     "iopub.status.busy": "2025-08-28T08:27:40.134998Z",
     "iopub.status.idle": "2025-08-28T08:27:43.825035Z",
     "shell.execute_reply": "2025-08-28T08:27:43.824078Z",
     "shell.execute_reply.started": "2025-08-28T08:27:40.135354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      " Classical  Mesenchymal  Neural  Proneural\n",
      "        43           48      28         43 \n",
      "\n",
      "Test set\n",
      " Classical  Mesenchymal  Neural  Proneural\n",
      "        14           16       9         15 \n",
      "\n",
      "Validation set\n",
      " Classical  Mesenchymal  Neural  Proneural\n",
      "        14           17       9         14 \n",
      "\n",
      "\n",
      "Count Samples: \n",
      "\n",
      "No unlabel data found!\n",
      " Train set  Test set  Validation set\n",
      "       162        54              54 \n",
      "\n",
      "No unlabel data found!\n",
      " Train set  Test set  Validation set\n",
      "       162        54              54 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_id in [3]:\n",
    "    tmp = list(LABEL_MAPPING_NAME)\n",
    "    label_files = ['tr', 'te', 'val']\n",
    "    dict = {\n",
    "        'tr': 'Train set',\n",
    "        'te': 'Test set',\n",
    "        'val': 'Validation set'\n",
    "    }\n",
    "    \n",
    "    for label_file in label_files:\n",
    "        df = pd.read_csv(f'{data_folder}/{fold_id}/labels_{label_file}.csv', header=None, names=['featname'])\n",
    "        feature_counts = df['featname'].value_counts().sort_index()\n",
    "        \n",
    "        print(f'{dict[label_file]}')\n",
    "        \n",
    "        res = {}\n",
    "        for feature, count in feature_counts.items():\n",
    "            res[tmp[feature]] = count\n",
    "\n",
    "        print(pd.DataFrame(res, index=[0]).to_string(index=False), '\\n')\n",
    "    \n",
    "    print('\\nCount Samples: \\n')\n",
    "    for idx in view_list:\n",
    "        res = {}\n",
    "        for label_file in label_files:\n",
    "            df = pd.read_csv(f'{data_folder}/{fold_id}/{idx}_{label_file}.csv', header=None, names=['featname'])\n",
    "            res[dict[label_file]] = df.shape[0]\n",
    "        \n",
    "        try:\n",
    "            unlabeled_df = pd.read_csv(f'{data_folder}/{fold_id}/{idx}_unlabeled.csv', header=None, names=['featname'])\n",
    "            res['Unlabeled Set'] = unlabeled_df.shape[0]\n",
    "        except:\n",
    "            print(f'No unlabel data found!')\n",
    "            \n",
    "        print(pd.DataFrame(res, index=[0]).to_string(index=False), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.1. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:43.826538Z",
     "iopub.status.busy": "2025-08-28T08:27:43.826269Z",
     "iopub.status.idle": "2025-08-28T08:27:43.835776Z",
     "shell.execute_reply": "2025-08-28T08:27:43.834850Z",
     "shell.execute_reply.started": "2025-08-28T08:27:43.826510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_eval(xtrain, ytrain, xtest, ytest, kk):\n",
    "    if len(kk) == 0:\n",
    "        kk = randint(1, np.size(xtrain, 1))\n",
    "    \n",
    "    sf = [i for i in range(np.size(xtrain, 1)) if kk[i] == 1]\n",
    "    pos = np.transpose(sf)\n",
    "\n",
    "    model = svm.SVC()\n",
    "    Xtrain = xtrain[:, pos]\n",
    "    Xtest = xtest[:, pos]\n",
    "    \n",
    "    model.fit(Xtrain, ytrain)\n",
    "    ypred = model.predict(Xtest)\n",
    "\n",
    "    return f1_score(ypred, ytest, average='macro') * 100\n",
    "\n",
    "def fit_all(xtrain, ytrain, xtest, ytest, kk):\n",
    "    res = []\n",
    "    for i in range(len(xtrain)):\n",
    "        res.append(fit_eval(xtrain[i], ytrain[i], xtest[i], ytest[i], kk))\n",
    "\n",
    "    return np.array(res).mean()\n",
    "\n",
    "def fit_all_print(xtrain, ytrain, xtest, ytest, kk):\n",
    "    res = []\n",
    "    for i in range(len(xtrain)):\n",
    "        res.append(fit_eval(xtrain[i], ytrain[i], xtest[i], ytest[i], kk))\n",
    "\n",
    "    print(res)\n",
    "    print(np.array(res).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:43.837414Z",
     "iopub.status.busy": "2025-08-28T08:27:43.837133Z",
     "iopub.status.idle": "2025-08-28T08:27:43.855098Z",
     "shell.execute_reply": "2025-08-28T08:27:43.853982Z",
     "shell.execute_reply.started": "2025-08-28T08:27:43.837385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Particle:\n",
    "        def __init__(self):\n",
    "            self.Position = None\n",
    "            self.Cost = None\n",
    "            self.Velocity = None\n",
    "            self.Best = {'Position': None, 'Cost': None}\n",
    "\n",
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def update_velocity_position(x, v, pbest, gbest, w=0.5, c1=1.5, c2=1.5):\n",
    "    r1 = np.random.rand()\n",
    "    r2 = np.random.rand()\n",
    "    v_new = w * v + c1 * r1 * (pbest - x) + c2 * r2 * (gbest - x)\n",
    "    x_new = np.where(np.random.rand(len(x)) < sigmoid(v_new), 1, 0)\n",
    "    return x_new, v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T09:15:03.347335Z",
     "iopub.status.busy": "2025-08-28T09:15:03.346939Z",
     "iopub.status.idle": "2025-08-28T09:15:03.510997Z",
     "shell.execute_reply": "2025-08-28T09:15:03.509754Z",
     "shell.execute_reply.started": "2025-08-28T09:15:03.347295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def PSO1(nParticles, Max_FEs, EFs, Run, Xtrain, ytrain, Xval, yval, Xtest, ytest, Cost_tr, Cost_va, Cost_te, FN):\n",
    "    E = EFs\n",
    "    nVar = Xtrain[0].shape[1]  \n",
    "    VarSize = (1, nVar)    \n",
    "\n",
    "    VarMin = -3            \n",
    "    VarMax = 3             \n",
    "    lu_v = 3 * np.array([-np.ones(nVar), np.ones(nVar)])\n",
    "\n",
    "    nPop = nParticles              # Population Size (Swarm Size)\n",
    "    w = 0.7\n",
    "    c1 = 1.5              # Personal Learning Coefficient\n",
    "    c2 = 1.5                # Global Learning Coefficient\n",
    "\n",
    "    stagnation_counter = 0 # Parameters for stagnation detection\n",
    "    stagnation_threshold = 10  # Number of iterations without improvement to trigger mRMR\n",
    "\n",
    "    previous_best_cost = 0\n",
    "    mrmr_applied = False \n",
    "    particle = [Particle() for _ in range(nPop)] # Particle Initialization\n",
    "    GlobalBest = {'Position': None, 'Cost': 0} \n",
    "\n",
    "    for i in range(nPop):\n",
    "        particle[i].Velocity = np.random.uniform(VarMin, VarMax, VarSize)\n",
    "\n",
    "    for i in range(nVar):\n",
    "        particle[0].Velocity[0, i] = 3\n",
    "        particle[0].Position = np.ones(VarSize)\n",
    "    particle[0].Cost = fit_all(Xtrain, ytrain, Xval, yval, particle[0].Position[0])\n",
    "\n",
    "    for i in range(1, nPop):\n",
    "        SS = sigmoid(particle[i].Velocity)\n",
    "        R = np.random.rand(*VarSize)\n",
    "        particle[i].Position = (R < SS).astype(int)\n",
    "        particle[i].Cost = fit_all(Xtrain, ytrain, Xval, yval, particle[i].Position[0])\n",
    "\n",
    "        particle[i].Best['Position'] = particle[i].Position  # Update Personal Best\n",
    "        particle[i].Best['Cost'] = particle[i].Cost\n",
    "\n",
    "        if particle[i].Best['Cost'] > GlobalBest['Cost']: # Update Global Best\n",
    "            GlobalBest = particle[i].Best\n",
    "\n",
    "    particle[0].Best['Position'] = particle[0].Position\n",
    "    particle[0].Best['Cost'] = particle[0].Cost\n",
    "    \n",
    "    Fit_tr = fit_all(Xtrain, ytrain, Xtrain, ytrain, particle[0].Position[0])\n",
    "    GlobalBest = particle[0].Best\n",
    "    Fit_te = fit_all(Xtrain, ytrain, Xtest, ytest, particle[0].Position[0])\n",
    "    previous_best_cost = GlobalBest['Cost']\n",
    "\n",
    "    while EFs <= Max_FEs:\n",
    "        EFs += nPop\n",
    "\n",
    "        for i in range(nPop):\n",
    "            particle[i].Velocity = (w * particle[i].Velocity +\n",
    "                                    c1 * np.random.rand(*VarSize) * (particle[i].Best['Position'] - particle[i].Position) +\n",
    "                                    c2 * np.random.rand(*VarSize) * (GlobalBest['Position'] - particle[i].Position))\n",
    "\n",
    "            V_u = lu_v[1, :]\n",
    "            particle[i].Velocity = np.clip(particle[i].Velocity, -V_u, V_u)\n",
    "\n",
    "            SS = sigmoid(particle[i].Velocity)\n",
    "            R = np.random.rand(*VarSize)\n",
    "            particle[i].Position = (R < SS).astype(int)\n",
    "\n",
    "            particle[i].Cost = fit_all(Xtrain, ytrain, Xval, yval, particle[i].Position[0])\n",
    "\n",
    "            if particle[i].Cost > particle[i].Best['Cost']:\n",
    "                particle[i].Best['Position'] = particle[i].Position\n",
    "                particle[i].Best['Cost'] = particle[i].Cost\n",
    "\n",
    "                if particle[i].Best['Cost'] > GlobalBest['Cost']:\n",
    "                    Fit_tr = fit_all(Xtrain, ytrain, Xtrain, ytrain, particle[i].Position[0])\n",
    "                    GlobalBest = particle[i].Best\n",
    "                    Fit_te = fit_all(Xtrain, ytrain, Xtest, ytest, particle[i].Position[0])\n",
    "            \n",
    "            if E >= Max_FEs:\n",
    "                break\n",
    "            Cost_tr[E, Run - 1] = Fit_tr\n",
    "            Cost_va[E, Run - 1] = GlobalBest['Cost']\n",
    "            Cost_te[E, Run - 1] = Fit_te\n",
    "            E += 1\n",
    "\n",
    "        if GlobalBest['Cost'] > previous_best_cost:\n",
    "            previous_best_cost = GlobalBest['Cost']\n",
    "            stagnation_counter = 0\n",
    "        else:\n",
    "            stagnation_counter += 1\n",
    "\n",
    "        if stagnation_counter >= stagnation_threshold and not mrmr_applied:\n",
    "            print(f\"Stagnation detected at iteration {EFs}, applying mRMR to reintroduce features...\")\n",
    "            mrmr_applied = True\n",
    "            \n",
    "            unselected_features = np.where(GlobalBest['Position'][0] == 0)[0]\n",
    "            \n",
    "            if len(unselected_features) > 0:  # Only proceed if there are unselected features\n",
    "                mrmr_scores = []\n",
    "                \n",
    "                for fold_idx in range(len(Xtrain)):\n",
    "                    X_train_df = pd.DataFrame(Xtrain[fold_idx], columns=[f'feature_{i}' for i in range(Xtrain[fold_idx].shape[1])])\n",
    "                    y_data = np.squeeze(ytrain[fold_idx])\n",
    "                    y_train = pd.Series(y_data)\n",
    "                    \n",
    "                    selected_feature_names = mrmr_classif(X_train_df, y_train, K=min(100, X_train_df.shape[1]))\n",
    "                    feature_indices = [int(feature.split('_')[1]) for feature in selected_feature_names]\n",
    "                    mrmr_scores.extend(feature_indices)\n",
    "\n",
    "                feature_counts = Counter(mrmr_scores)\n",
    "                feature_counts_filtered = {feat: count for feat, count in feature_counts.items() \n",
    "                                         if feat in unselected_features}\n",
    "                k_reintroduce = min(20, len(feature_counts_filtered))\n",
    "                \n",
    "                if k_reintroduce > 0:\n",
    "                    features_to_reintroduce = [feat for feat, _ in \n",
    "                                              sorted(feature_counts_filtered.items(), \n",
    "                                                     key=lambda x: x[1], \n",
    "                                                     reverse=True)[:k_reintroduce]]\n",
    "                    print(f\"Add {len(features_to_reintroduce)} features back to PSO to continue\")\n",
    "                    temp_position = GlobalBest['Position'][0].copy()\n",
    "                    for feat in features_to_reintroduce:\n",
    "                        temp_position[feat] = 1\n",
    "\n",
    "                    temp_cost = fit_all(Xtrain, ytrain, Xval, yval, temp_position)\n",
    "\n",
    "                    if temp_cost > GlobalBest['Cost']:\n",
    "                        GlobalBest['Position'][0] = temp_position\n",
    "                        GlobalBest['Cost'] = temp_cost\n",
    "                        Fit_tr = fit_all(Xtrain, ytrain, Xtrain, ytrain, temp_position)\n",
    "                        Fit_te = fit_all(Xtrain, ytrain, Xtest, ytest, temp_position)\n",
    "                        stagnation_counter = 0\n",
    "                        \n",
    "                        print(f\"mRMR reintroduced {len(features_to_reintroduce)} features. New F1-score: {GlobalBest['Cost']}\")\n",
    "                    else:\n",
    "                        print(\"mRMR feature reintroduction did not improve performance.\")\n",
    "\n",
    "        # print(f\"SFE-PSO : Function Evaluation: {EFs} F1-macro = {GlobalBest['Cost']} \"\n",
    "        #       f\"Number of Selected Features = {np.sum(GlobalBest['Position'])} Run: {Run}\")\n",
    "\n",
    "    FN[Run - 1, 0] = np.sum(GlobalBest['Position'])\n",
    "    return Cost_tr, Cost_va, Cost_te, FN, GlobalBest['Position'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:27:43.891760Z",
     "iopub.status.busy": "2025-08-28T08:27:43.891454Z",
     "iopub.status.idle": "2025-08-28T08:27:47.170620Z",
     "shell.execute_reply": "2025-08-28T08:27:47.169750Z",
     "shell.execute_reply.started": "2025-08-28T08:27:43.891730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_data(fold_id, file : str):\n",
    "    return pd.read_csv(f'{data_folder}/{fold_id}/{file}.csv', delimiter=\",\", header=None)\n",
    "\n",
    "feat1 = get_data(1, '1_featname')\n",
    "feat2 = get_data(1, '2_featname')\n",
    "feat = np.concatenate((feat1, feat2), axis=0)\n",
    "\n",
    "Xtrain = []\n",
    "Xval = []\n",
    "Xtest = []\n",
    "\n",
    "ytrain = []\n",
    "yval = []\n",
    "ytest = []\n",
    "\n",
    "for i in range(1, num_models+1):\n",
    "    inp = np.concatenate((get_data(i, '1_tr'), get_data(i, '2_tr')), axis=1)\n",
    "    Input = np.asarray(inp)\n",
    "    Input = stats.zscore(Input)\n",
    "\n",
    "    tar = get_data(i, 'labels_tr')\n",
    "    Target = np.asarray(tar)\n",
    "           \n",
    "    Xtrain.append(Input)\n",
    "    ytrain.append(Target)\n",
    "           \n",
    "    inp = np.concatenate((get_data(i, '1_val'), get_data(i, '2_val')), axis=1)\n",
    "    Input = np.asarray(inp)\n",
    "    Input = stats.zscore(Input)\n",
    "\n",
    "    tar = get_data(i, 'labels_val')\n",
    "    Target = np.asarray(tar)\n",
    "           \n",
    "    Xval.append(Input)\n",
    "    yval.append(Target)\n",
    "           \n",
    "    inp = np.concatenate((get_data(i, '1_te'), get_data(i, '2_te')), axis=1)\n",
    "    Input = np.asarray(inp)\n",
    "    Input = stats.zscore(Input)\n",
    "\n",
    "    tar = get_data(i, 'labels_te')\n",
    "    Target = np.asarray(tar)\n",
    "           \n",
    "    Xtest.append(Input)\n",
    "    ytest.append(Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T09:15:09.550333Z",
     "iopub.status.busy": "2025-08-28T09:15:09.549964Z",
     "iopub.status.idle": "2025-08-28T09:37:15.577363Z",
     "shell.execute_reply": "2025-08-28T09:37:15.575998Z",
     "shell.execute_reply.started": "2025-08-28T09:15:09.550300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stagnation detected at iteration 925, applying mRMR to reintroduce features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.25it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.27it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.29it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mRMR feature reintroduction did not improve performance.\n",
      "Stagnation detected at iteration 861, applying mRMR to reintroduce features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.22it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.23it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.21it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mRMR feature reintroduction did not improve performance.\n",
      "Stagnation detected at iteration 981, applying mRMR to reintroduce features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.30it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.34it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.24it/s]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mRMR feature reintroduction did not improve performance.\n",
      "Stagnation detected at iteration 892, applying mRMR to reintroduce features...\n"
     ]
    }
   ],
   "source": [
    "cnt_feat = 400\n",
    "Max_FEs = 1000\n",
    "Max_Run = 20\n",
    "\n",
    "UR = 0.3\n",
    "UR_Max = 0.3\n",
    "UR_Min = 0.001\n",
    "Run = 1\n",
    "\n",
    "Best_Score = 0\n",
    "Best_X = None\n",
    "Cost_tr = np.full([Max_FEs, Max_Run], np.nan)\n",
    "Cost_va = np.full([Max_FEs, Max_Run], np.nan)\n",
    "Cost_te = np.full([Max_FEs, Max_Run], np.nan)\n",
    "Sel_feat = np.full([Max_FEs, Max_Run], np.nan)\n",
    "FN = np.zeros([Max_Run, 1])\n",
    "Fit_X = 0\n",
    "\n",
    "w = 0.5   # inertia weight\n",
    "c1 = 1.5  # cognitive parameter\n",
    "c2 = 1.5  # social parameter\n",
    "n_particles=30 # the number of particles\n",
    "\n",
    "while Run <= Max_Run:\n",
    "    EFs = 1\n",
    "\n",
    "    X = np.random.randint(1, 2, np.size(Input, 1))   # Initialize an Individual X\n",
    "    Fit_tr = fit_all(Xtrain, ytrain, Xtrain, ytrain, X)    # Calculate the Fitness of X\n",
    "    Fit_X = fit_all(Xtrain, ytrain, Xval, yval, X)    # Calculate the Fitness of X\n",
    "    Fit_te = fit_all(Xtrain, ytrain, Xtest, ytest, X)    # Calculate the Fitness of X\n",
    "    Nvar = np.size(Input, 1)                         # Number of Features in Dataset\n",
    "\n",
    "    while EFs <= Max_FEs:\n",
    "        \"\"\"*************************SFE Algorithm**********************************\"\"\"\n",
    "        X_New = np.copy(X)\n",
    "\n",
    "        # Non-selection operation:\n",
    "        U_Index = np.where(X == 1)                      # Find Selected Features in X\n",
    "        NUSF_X = np.size(U_Index, 1)                    # Number of Selected Features in X\n",
    "        UN = math.ceil(UR * Nvar)                         # The Number of Features to Unselect: Eq(2)\n",
    "        K1 = np.random.randint(0, NUSF_X, UN)           # Generate UN random number between 1 to the number of selected features in X\n",
    "        res = np.array([*set(K1)])\n",
    "        res1 = np.array(res)\n",
    "        K = U_Index[0][[res1]]                          # K=index(U)\n",
    "        X_New[K] = 0                                    # Set X_New (K)=0 \n",
    "\n",
    "       # Selection operation:\n",
    "        if np.sum(X_New) < cnt_feat:\n",
    "            S_Index = np.where(X_New == 0)              # Find non-selected Features in X\n",
    "            NSF_X = np.size(S_Index, 1)                 # Number of non-selected Features in X\n",
    "            SN = cnt_feat - np.sum(X_New)               # The Number of Features to Select\n",
    "            res = np.random.choice(np.arange(NSF_X), size=SN, replace=False)        # Generate SN random number between 1 to the number of non-selected features in X\n",
    "            res1 = np.array(res)\n",
    "            K = S_Index[0][[res1]]\n",
    "#                 X_New = np.copy(X)\n",
    "            X_New[K] = 1                                # Set X_New (K)=1\n",
    "\n",
    "        Fit_X_New = fit_all(Xtrain, ytrain, Xval, yval, X_New) # Calculate the Fitness of X_New\n",
    "\n",
    "        if Fit_X_New > Fit_X:\n",
    "            X = np.copy(X_New)\n",
    "            Fit_X = Fit_X_New\n",
    "            Fit_tr = fit_all(Xtrain, ytrain, Xtrain, ytrain, X_New)\n",
    "            Fit_te = fit_all(Xtrain, ytrain, Xtest, ytest, X_New)\n",
    "\n",
    "        UR = (UR_Max - UR_Min) * ((Max_FEs - EFs) / Max_FEs) + UR_Min  # Eq(3)\n",
    "        Cost_tr[EFs-1, Run-1] = Fit_tr\n",
    "        Cost_va[EFs-1, Run-1] = Fit_X\n",
    "        Cost_te[EFs-1, Run-1] = Fit_te\n",
    "        Sel_feat[EFs-1, Run-1] = np.sum(X)\n",
    "\n",
    "        # if EFs % 100 == 0:\n",
    "        #     print('SFE - Iteration = {} :   F1-Score = {} :   Number of Selected Features= {} :  Run= {}'.format( EFs, Fit_X, np.sum(X), Run))\n",
    "        \n",
    "        EFs += 1\n",
    "\n",
    "        if Fit_X > Best_Score:\n",
    "            Best_Score = Fit_X\n",
    "            Best_X = X\n",
    "            \n",
    "        \"\"\"*************************PSO Algorithm**********************************\"\"\"\n",
    "        if EFs > 500 and EFs <= Max_FEs and (Cost_va[EFs-2,Run-1] - Cost_va[EFs-102,Run-1]) < 1e-2:\n",
    "            \"\"\"S = removing unselected features from the training dataset\"\"\"\n",
    "            S = np.array([i for i in range(len(X)) if X[i] == 1])\n",
    "            Xtrain_selected = [x[:, S] for x in Xtrain]\n",
    "            Xval_selected = [x[:, S] for x in Xval]\n",
    "            Xtest_selected = [x[:, S] for x in Xtest]\n",
    "            \n",
    "            Cost_tr, Cost_va, Cost_te, FN, gbest = PSO1(30, Max_FEs, EFs, Run, Xtrain_selected, ytrain, Xval_selected, yval, Xtest_selected, ytest, Cost_tr, Cost_va, Cost_te, FN)\n",
    "            gbest_fitness = FN[Run - 1, 0]\n",
    "            EFs = Max_FEs + 1\n",
    "                \n",
    "            if gbest_fitness > Best_Score:\n",
    "                Best_Score = gbest_fitness\n",
    "                Best_X = np.zeros(len(X))\n",
    "                cnt = 0\n",
    "                for i in range(len(X)):\n",
    "                    if X[i] == 1:\n",
    "                        if gbest[cnt] == 1:\n",
    "                            Best_X[i] = 1\n",
    "                        cnt += 1\n",
    "                        \n",
    "    Run += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.272353Z",
     "iopub.status.idle": "2025-08-28T08:32:11.272776Z",
     "shell.execute_reply": "2025-08-28T08:32:11.272577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to get the last non-NaN value in each column\n",
    "def get_last_valid_per_column(matrix):\n",
    "    last_valid_values = np.full(matrix.shape[1], np.nan)  # Initialize with NaN\n",
    "    for col in range(matrix.shape[1]):\n",
    "        valid_indices = np.where(~np.isnan(matrix[:, col]))[0]  # Get indices of non-NaN values\n",
    "        if valid_indices.size > 0:\n",
    "            last_valid_values[col] = matrix[valid_indices[-1], col]  # Get the last valid value\n",
    "    return last_valid_values\n",
    "\n",
    "# Get the last non-NaN value for each column in Cost_te\n",
    "cost1 = get_last_valid_per_column(Cost_te)\n",
    "\n",
    "valid_cost1 = cost1[~np.isnan(cost1)]  # Only consider non-empty values\n",
    "print('\\n Fitness evaluate on Test:')\n",
    "print('\\n Best = {} '.format(max(valid_cost1)))\n",
    "print('\\n Count = {} '.format(np.sum(Best_X)))\n",
    "print('\\n Mean = {} '.format(valid_cost1.mean()))\n",
    "print('\\n Worst = {}  '.format(min(valid_cost1)))\n",
    "print('\\n std  = {}'.format(np.std(valid_cost1)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(4, 5), dpi=300)\n",
    "\n",
    "valid_Cost_tr = Cost_tr[~np.isnan(Cost_tr).any(axis=1)]\n",
    "valid_Cost_va = Cost_va[~np.isnan(Cost_va).any(axis=1)]\n",
    "valid_Cost_te = Cost_te[~np.isnan(Cost_te).any(axis=1)]\n",
    "valid_Sel_feat = Sel_feat[~np.isnan(Sel_feat).any(axis=1)]\n",
    "\n",
    "ax1.plot(np.nanmean(valid_Cost_tr, axis=1), '-', label='Train', color='blue')\n",
    "ax1.plot(np.nanmean(valid_Cost_va, axis=1), '-', label='Validation', color='green')\n",
    "ax1.plot(np.nanmean(valid_Cost_te, axis=1), '-', label='Test', color='red')\n",
    "ax1.set_xlabel('Number of Fitness Evaluations')\n",
    "ax1.set_ylabel('Classification F1-Score')\n",
    "ax1.set_title('Classification F1-Score vs. Fitness Evaluations')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(np.nanmean(valid_Sel_feat, axis=1), '-', label='Num_feat', color='orange')\n",
    "ax2.set_xlabel('Number of Fitness Evaluations')\n",
    "ax2.set_ylabel('Number of Selected Features')\n",
    "ax2.set_title('Selected Features vs. Fitness Evaluations')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "# ax3.plot(np.nanmean(valid_Cost_tr, axis=1), '-', label='Train', color='blue')\n",
    "# ax3.plot(np.nanmean(valid_Cost_va, axis=1), '-', label='Validation', color='green')\n",
    "# ax3.set_xlabel('Number of Fitness Evaluations')\n",
    "# ax3.set_ylabel('Classification F1-Score')\n",
    "# ax3.set_title('Classification F1-Score vs. Fitness Evaluations')\n",
    "# ax3.grid()\n",
    "# ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.277583Z",
     "iopub.status.idle": "2025-08-28T08:32:11.277982Z",
     "shell.execute_reply": "2025-08-28T08:32:11.277780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loc_file_json_id_omic = data_folder + '/1/dict_id_omics.json'\n",
    "with open(loc_file_json_id_omic) as file_json_id_omic:\n",
    "    dct_OMIC_MAPPING_NAME = json.load(file_json_id_omic)\n",
    "    dct_OMIC_MAPPING_NAME = {int(k): v for k,v in dct_OMIC_MAPPING_NAME.items()} \n",
    "\n",
    "print(dct_OMIC_MAPPING_NAME)\n",
    "ROOT_DATA_FOLDER = f'/kaggle/input/{dataset_name}/{COHORT}/train_test_split_org/'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "LIST_OMICS = list(dct_OMIC_MAPPING_NAME.values())\n",
    "print(LIST_OMICS)\n",
    "LIST_OMICS_ID = np.arange(1,len(LIST_OMICS)+1,1)\n",
    "LIST_EXP_OMICS = ['_'.join(LIST_OMICS)]\n",
    "print(LIST_EXP_OMICS)\n",
    "\n",
    "\n",
    "LIST_TYPE_DATA = ['train', 'test']\n",
    "DATA_FOLDER = {'train': ROOT_DATA_FOLDER,\n",
    "              'test': ROOT_DATA_FOLDER}\n",
    "\n",
    "loc_file_json_id_label = data_folder + '/1/dct_index_subtype.json'\n",
    "with open(loc_file_json_id_label) as file_json_id_label:\n",
    "    dct_LABEL_MAPPING_NAME = json.load(file_json_id_label)\n",
    "    dct_LABEL_MAPPING_NAME = {int(k): v for k,v in dct_LABEL_MAPPING_NAME.items()}\n",
    "ORIGINAL_MAPPING_NAME = dct_LABEL_MAPPING_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.278906Z",
     "iopub.status.idle": "2025-08-28T08:32:11.279318Z",
     "shell.execute_reply": "2025-08-28T08:32:11.279127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BIOMARKERS_RESULT_FOLDER = '/kaggle/working/biomarkers'\n",
    "list_loc_biomarkers = []\n",
    "for dirname, _, filenames in os.walk(BIOMARKERS_RESULT_FOLDER):\n",
    "    for filename in filenames:\n",
    "        list_loc_biomarkers.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.1. Overlap with other genes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.280194Z",
     "iopub.status.idle": "2025-08-28T08:32:11.280584Z",
     "shell.execute_reply": "2025-08-28T08:32:11.280383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "direct_evidence_gene_list = [\n",
    "    'ABCB1', 'ABCC1', 'ABCG2', 'ACTA2', 'AKT1', 'ALDOA', 'AR', 'ATM', 'AURKA', 'BARD1', \n",
    "    'BCL2', 'BIRC5', 'BMP2', 'BRCA1', 'BRCA2', 'BRIP1', 'CASP7', 'CASP8', 'CAT', 'CAV1', \n",
    "    'CCND1', 'CCNE1', 'CDH1', 'CDH2', 'CDKN1B', 'CHEK1', 'CHEK2', 'CPT1A', 'CSF3', \n",
    "    'CTNNB1', 'CXCL2', 'CXCL8', 'CXCR4', 'CYP19A1', 'CYP1A1', 'CYP1B1', 'CYP2D6', 'CYP3A4', \n",
    "    'DDIT3', 'DNMT1', 'E2F1', 'EFNA1', 'EFEMP1', 'EGF', 'EGFR', 'ERBB2', 'ERBB3', 'ESR1', \n",
    "    'ESR2', 'F3', 'FASN', 'FN1', 'FOS', 'GJA1', 'GPX1', 'GSK3B', 'GSTP1', 'H2AX', 'HIF1A', \n",
    "    'HMOX1', 'HMMR', 'IFNG', 'IGF1', 'IGF1R', 'IL1B', 'IL6', 'JUN', 'KRAS', 'MDM2', 'MKI67', \n",
    "    'MMP1', 'MMP2', 'MMP3', 'MMP9', 'MTOR', 'NFKBIA', 'NOS2', 'NOS3', 'NOTCH1', 'NQO1', \n",
    "    'NQO2', 'PALB2', 'PARP1', 'PGR', 'PHGDH', 'PHB', 'PIK3CA', 'PPM1D', 'PTEN', 'PTGS2', \n",
    "    'RAD51', 'RAD51A', 'RAD54L', 'RARA', 'RB1', 'RB1CC1', 'RELA', 'SERPINB2', 'SLC22A1L', \n",
    "    'SLC2A1', 'SNAI1', 'SOD2', 'SPP1', 'STAT3', 'STMN1', 'TERT', 'TFRC', 'TNF', 'TOP2A', \n",
    "    'TP53', 'TRP53', 'TSG101', 'UBE2C', 'XRCC3'\n",
    "]\n",
    "# topn_lst = [50, 100, 200, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.281525Z",
     "iopub.status.idle": "2025-08-28T08:32:11.281904Z",
     "shell.execute_reply": "2025-08-28T08:32:11.281714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for file_loc in list_loc_biomarkers:\n",
    "    file_name = file_loc.split('/')[-1]\n",
    "    ig_biomarkers = pd.read_csv(file_loc).iloc[:, 0].str.split(r'\\|').str[0].values.tolist()\n",
    "    baseline_name = '-'.join(file_name.split('_')[:])\n",
    "#     for i in topn_lst:\n",
    "    intersect_direct = sorted(list(set(direct_evidence_gene_list).intersection(set(ig_biomarkers[:]))))\n",
    "    intersect_direct_str = ', '.join(intersect_direct)\n",
    "    data.append([baseline_name, len(ig_biomarkers), intersect_direct_str]) #, intersect_reference, intersect_inference])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Baseline', 'TopN', 'Intersect with Direct Evidence Gene List']) #, 'Intersect with Top Reference Gene List', 'Intersect with Top Inference Gene List'])\n",
    "\n",
    "# Print table\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(df)\n",
    "\n",
    "df.to_csv(\"/kaggle/working/genes_overlap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.2. Classic ML models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.282869Z",
     "iopub.status.idle": "2025-08-28T08:32:11.283309Z",
     "shell.execute_reply": "2025-08-28T08:32:11.283112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('max_colwidth', None)\n",
    "def printmd(string, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.284163Z",
     "iopub.status.idle": "2025-08-28T08:32:11.284588Z",
     "shell.execute_reply": "2025-08-28T08:32:11.284382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_classification_report(\n",
    "    n_class,\n",
    "    label,\n",
    "    pred,\n",
    "    label_mapping_name,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    annot=True,\n",
    "    path=None,  # str path to save fig. If not None\n",
    "    shown=True,\n",
    "):\n",
    "\n",
    "    clf_report = classification_report(\n",
    "        label,\n",
    "        pred,\n",
    "        target_names=label_mapping_name,\n",
    "        digits=4,\n",
    "        zero_division=0,\n",
    "        output_dict=True,\n",
    "    )\n",
    "\n",
    "    clf_df = pd.DataFrame(clf_report)\n",
    "    clf_df.loc[[\"precision\", \"recall\"], \"accuracy\"] = np.nan\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(12)\n",
    "    ConfusionMatrixDisplay(\n",
    "        confusion_matrix(label, pred), display_labels=label_mapping_name\n",
    "    ).plot(cmap=cmap, ax=ax1)\n",
    "    sns.heatmap(\n",
    "        clf_df.iloc[:-1, :].T, annot=annot, cmap=cmap, robust=True, ax=ax2, fmt=fmt\n",
    "    )\n",
    "    if path is not None:\n",
    "        fig.savefig(path, dpi=300)\n",
    "    if shown:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.285576Z",
     "iopub.status.idle": "2025-08-28T08:32:11.285980Z",
     "shell.execute_reply": "2025-08-28T08:32:11.285780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tuning_and_eval(gridcvs, X_train, y_train, X_test, y_test,\n",
    "                    scoring, refit, is_binary_problem,\n",
    "                    result_on_dataset, rank_hparams_info):\n",
    "    assert 'test' in result_on_dataset\n",
    "    assert isinstance(rank_hparams_info, bool)\n",
    "    ###\n",
    "    lst_dct_result = [] # to return\n",
    "    ###\n",
    "\n",
    "    start=datetime.now()\n",
    "    X = {}\n",
    "    y = {}\n",
    "    X['train'] = X_train\n",
    "    X['test'] = X_test\n",
    "    y['train'] = y_train\n",
    "    y['test'] = y_test\n",
    "\n",
    "    for model_name, gs_est in sorted(gridcvs.items()):\n",
    "        ###\n",
    "        sub_result = {}\n",
    "        sub_result['model'] = model_name\n",
    "        ###\n",
    "\n",
    "        start_individual_type_model = datetime.now()\n",
    "        gs_est.fit(X['train'],y['train'])\n",
    "\n",
    "        ###\n",
    "        sub_result['best_params'] = gs_est.best_params_\n",
    "        ###\n",
    "\n",
    "        ###\n",
    "        sub_result[f'best_tuning_{refit}'] = gs_est.best_score_ * 100\n",
    "        sub_result['best_tuning_std'] = gs_est.cv_results_[f'std_test_{refit}'][gs_est.best_index_] * 100\n",
    "        ###\n",
    "\n",
    "        if rank_hparams_info:\n",
    "            select_result_cols = []\n",
    "            for metric in scoring:\n",
    "                select_result_cols.extend(['rank_test_'+metric,'mean_test_'+ metric, 'std_test_'+metric])\n",
    "            select_result_cols.extend(['params'])\n",
    "\n",
    "            dataframe_results = pd.DataFrame(gs_est.cv_results_).loc[:,select_result_cols].sort_values(by=f'mean_test_{refit}',ascending=False)\n",
    "            display(dataframe_results[:10])\n",
    "\n",
    "        for type_data in result_on_dataset:\n",
    "            y_predict = gs_est.predict(X[type_data])\n",
    "\n",
    "            acc = accuracy_score(y_true=y[type_data], y_pred=y_predict)\n",
    "\n",
    "            ###\n",
    "            sub_result[f'{type_data}_acc'] = acc * 100\n",
    "            ###\n",
    "\n",
    "            if is_binary_problem:\n",
    "                f1 = f1_score(y_true=y[type_data], y_pred=y_predict,average='binary')\n",
    "                y_score = gs_est.predict_proba(X[type_data])[:, 1]\n",
    "                roc_auc = roc_auc_score(y_true=y[type_data], y_score=y_score)\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_f1'] = f1 * 100\n",
    "                ###\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_roc_auc'] = roc_auc * 100\n",
    "                ###\n",
    "            else:\n",
    "                f1_macro = f1_score(y_true=y[type_data], y_pred=y_predict,average='macro')\n",
    "                f1_weighted = f1_score(y_true=y[type_data], y_pred=y_predict,average='weighted')\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_f1_macro'] = f1_macro * 100\n",
    "                ###\n",
    "\n",
    "                ###\n",
    "                sub_result[f'{type_data}_f1_weighted'] = f1_weighted * 100\n",
    "                ##\n",
    "\n",
    "            ###\n",
    "            lst_dct_result.append(sub_result)\n",
    "            ###\n",
    "\n",
    "            pd_cfm = pd.crosstab(\n",
    "                y[type_data]\n",
    "                , y_predict\n",
    "                , margins=True\n",
    "                , rownames=['True label']\n",
    "                , colnames=['Pred label']\n",
    "            )\n",
    "            \n",
    "            # Đảm bảo các label thiếu có mặt trong chỉ mục và cột của bảng crosstab\n",
    "            labels = list(ORIGINAL_MAPPING_NAME.values())\n",
    "            pd_cfm = pd_cfm.reindex(index=labels + ['All'], columns=labels + ['All'], fill_value=0)\n",
    "\n",
    "            folder_save_fig = f\"/kaggle/working/cfm/{biomarker_file.split('/')[-1].split('.')[-2]}/{model_name}\"\n",
    "            if not os.path.exists(folder_save_fig):\n",
    "                os.makedirs(folder_save_fig)\n",
    "            path_save_fig = f\"{folder_save_fig}/top{threshold}.png\"\n",
    "            display_classification_report(n_class=len(ORIGINAL_MAPPING_NAME)\n",
    "                                          , label=y[type_data]\n",
    "                                          , pred= y_predict\n",
    "                                          , label_mapping_name=ORIGINAL_MAPPING_NAME.values()\n",
    "                                          , path=path_save_fig\n",
    "                                          , shown=False\n",
    "                                         )\n",
    "    return lst_dct_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.287239Z",
     "iopub.status.idle": "2025-08-28T08:32:11.287635Z",
     "shell.execute_reply": "2025-08-28T08:32:11.287450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_biomarker(dict_X_train, dict_y_train, dict_X_test, dict_y_test,\n",
    "                       omics=['GE_CNA', 'GE','CNA'], random_state = RANDOM_STATE,\n",
    "                       result_on_dataset = ['train','test'], rank_hparams_info = True,\n",
    "                       is_binary_problem=False):\n",
    "    assert 'test' in result_on_dataset\n",
    "    assert isinstance(rank_hparams_info, bool)\n",
    "    ###\n",
    "    validate_biomarker_result = []\n",
    "    ###\n",
    "    scoring = None\n",
    "    refit= None\n",
    "    if is_binary_problem:\n",
    "        scoring = ['f1','accuracy','roc_auc']\n",
    "        refit = 'f1'\n",
    "    else: \n",
    "        scoring = ['f1_macro','f1_weighted', 'accuracy']\n",
    "        refit = 'f1_macro'\n",
    "\n",
    "    # Initializing classifiers\n",
    "    clf1 = LogisticRegression(random_state=random_state, max_iter=10000, n_jobs=-1)\n",
    "\n",
    "    # Binary case, probability = True to cal ROC_AUC, slowdown k-fold....\n",
    "    clf2 = SVC(random_state=random_state, probability=is_binary_problem)\n",
    "\n",
    "    clf3 = RandomForestClassifier(random_state=random_state,n_jobs=-1)\n",
    "\n",
    "    # Building the pipelines\n",
    "    pipe1 = Pipeline([('std', 'passthrough'),\n",
    "                      ('clf1', clf1)])\n",
    "\n",
    "    pipe2 = Pipeline([('std', 'passthrough'),\n",
    "                      ('clf2', clf2)])\n",
    "\n",
    "#     # only apply std to mRNA data/ BY index mRNA| ignore or passthorough not to\n",
    "#     # apply standard scaler to remaining index corresponding to CNA data\n",
    "#     column_trans = ColumnTransformer(\n",
    "#         [('scaler', StandardScaler(),list(range(len(GENE['mRNA']))))]\n",
    "#         ,remainder='passthrough')\n",
    "    # Setting up the parameter grids\n",
    "    param_grid1 = [{\n",
    "                    'std': [MinMaxScaler()],\n",
    "                    'clf1__penalty': ['l2'],\n",
    "                    'clf1__multi_class':[\"multinomial\"],\n",
    "                    'clf1__solver':[\"newton-cg\"],\n",
    "                    'clf1__class_weight': [\"balanced\"],\n",
    "                    'clf1__C': np.power(10., np.arange(-4, 3)),\n",
    "                    }]\n",
    "\n",
    "    param_grid2 = [{\n",
    "                    'std': [MinMaxScaler()],\n",
    "                    'clf2__kernel': ['rbf'],\n",
    "                    'clf2__class_weight': [\"balanced\"],\n",
    "                    'clf2__C': np.power(10., np.arange(-4, 3)),\n",
    "                    'clf2__gamma': list(np.power(10., np.arange(-4, 0))) + ['scale']\n",
    "                    }]\n",
    "\n",
    "    param_grid3 = [{'n_estimators': [50, 100, 150],\n",
    "                    'max_features': [\"sqrt\"],\n",
    "                    'max_depth' : list(range(1, 10)) + [None],\n",
    "                    'criterion' :[\"gini\"],\n",
    "                    'class_weight': [\"balanced\", \"balanced_subsample\"]}]\n",
    "\n",
    "    # Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "    gridcvs = {}\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=10, random_state=random_state)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=5, random_state=random_state)\n",
    "\n",
    "    train_options = zip(\n",
    "                        (param_grid1,\n",
    "                         param_grid2,\n",
    "                         param_grid3,\n",
    "                        ),\n",
    "                        (pipe1,\n",
    "                         pipe2,\n",
    "                         clf3,\n",
    "                        ),\n",
    "                        ('1_Softmax',\n",
    "                         '2_SVM',\n",
    "                         '3_RandomForest',\n",
    "                        )\n",
    "                       )\n",
    "\n",
    "    for pgrid, est, model_name in train_options:\n",
    "        gcv = GridSearchCV(estimator=est,\n",
    "                           param_grid=pgrid,\n",
    "                           scoring=scoring,\n",
    "                           n_jobs=-1,\n",
    "                           cv=cv,\n",
    "                           verbose=0,\n",
    "                           refit=refit)\n",
    "        gridcvs[model_name] = gcv\n",
    "\n",
    "    for omic in omics:\n",
    "#         print('-'*100)\n",
    "#         printmd(f'Validate on {omic} data:\\n', color=\"red\")\n",
    "\n",
    "        X_train = dict_X_train[omic]\n",
    "        y_train = np.array(dict_y_train[omic], dtype=np.int16)\n",
    "#         print('Train dist: ', np.unique(y_train, return_counts=True ))\n",
    "\n",
    "        X_test = dict_X_test[omic]\n",
    "        y_test = np.array(dict_y_test[omic], dtype=np.int16)\n",
    "#         print('Test dist', np.unique(y_test, return_counts=True ),'\\n')\n",
    "\n",
    "        # run tuning and eval\n",
    "        tmp_lst_dct_tuning_result = tuning_and_eval(gridcvs, X_train, y_train, X_test, y_test,\\\n",
    "                        scoring, refit,is_binary_problem,\n",
    "                        result_on_dataset, rank_hparams_info)\n",
    "        ###\n",
    "        tmp_base= {'using_omic': omic}\n",
    "        validate_biomarker_result.extend([copy.deepcopy(tmp_base) for i in range(len(tmp_lst_dct_tuning_result))])\n",
    "        for dct_tmp, dct_val in zip(tmp_lst_dct_tuning_result, validate_biomarker_result):\n",
    "            dct_val.update(dct_tmp)\n",
    "        ###\n",
    "    return validate_biomarker_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.288686Z",
     "iopub.status.idle": "2025-08-28T08:32:11.289115Z",
     "shell.execute_reply": "2025-08-28T08:32:11.288883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(biomarker_file, threshold):\n",
    "    # init var and para to save the result\n",
    "    ###\n",
    "    validate_result=[]\n",
    "    base_result = {}\n",
    "    ###\n",
    "#     dct_data_structure = {col: [] for col in lst_cols}\n",
    "#     pd_result = pd.DataFrame(columns=lst_cols)\n",
    "\n",
    "    # Read data\n",
    "    dict_df_label = {}\n",
    "    dict_df_data = {}\n",
    "    # Read data as df and create numpy array data for labeled data\n",
    "    for type_data in LIST_TYPE_DATA:\n",
    "        # modified\n",
    "        dict_df_label[type_data] = pd.read_csv(DATA_FOLDER[type_data] + f'labels_{type_data[:2]}.csv', names=['disease_subtypes'])\n",
    "        # ---------------------------------\n",
    "\n",
    "        # added\n",
    "        dict_df_label[type_data]['disease_subtypes'] = dict_df_label[type_data]['disease_subtypes'].astype('int')\n",
    "        dict_df_label[type_data].index.names = ['sampleID']\n",
    "        \n",
    "        dict_df_label[type_data].replace({'disease_subtypes': ORIGINAL_MAPPING_NAME}, inplace=True)\n",
    "        # ---------------------------------\n",
    "        \n",
    "        \n",
    "        dict_df_omics = {}\n",
    "        dict_narray_omics = {}\n",
    "        for omic in LIST_OMICS:\n",
    "            # added\n",
    "            tmp_feat_name = pd.read_csv(DATA_FOLDER[type_data]+ f'{LIST_OMICS.index(omic)+1}_featname.csv', names=['feat_name'])\n",
    "#             tmp_feat_name['feat_name'] = tmp_feat_name['feat_name'].str.split('|').str[0]\n",
    "            lst_name = tmp_feat_name.values.reshape(-1).tolist()\n",
    "            # ---------------------------------\n",
    "\n",
    "            # modified\n",
    "            dict_df_omics[omic] = pd.read_csv(DATA_FOLDER[type_data] + f'{LIST_OMICS.index(omic)+1}_{type_data[:2]}.csv',names=lst_name)\n",
    "            # ---------------------------------\n",
    "\n",
    "        dict_df_data[type_data] = dict_df_omics\n",
    "\n",
    "    LABEL_MAPPING_NAME = dict_df_label['train']['disease_subtypes'].astype('category').cat.categories # sorted by alphabetical order\n",
    "#     print('LABEL_MAPPING_NAME', LABEL_MAPPING_NAME)\n",
    "    \n",
    "    # Convert categorical label to numerical label\n",
    "    for type_data in LIST_TYPE_DATA:\n",
    "        dict_df_label[type_data].loc[:,'disease_subtypes'] = dict_df_label[type_data]['disease_subtypes'].astype('category').cat.codes\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Keep only biomarker genes found from TCGA data\n",
    "#     print('-'*100)\n",
    "#     print('KEEP ONLY BIOMARKER GENES FOUND FROM TCGA DATA')\n",
    "    score_genes = pd.read_csv(biomarker_file)\n",
    "    score_genes = score_genes.iloc[:, 0]\n",
    "    top_genes = list(set(score_genes.to_numpy(copy=True).reshape(-1)))\n",
    "#     print(top_genes)\n",
    "    top_genes = [gene.upper() for gene in top_genes]\n",
    "#     print(f'Top {threshold} from TCGA have {len(top_genes)} unique genes/features:')\n",
    "    ###\n",
    "    base_result['n_unq_markers'] = len(top_genes)\n",
    "    base_result['lst_unq_markers'] = top_genes\n",
    "    ###\n",
    "    \n",
    "    GENE = {}\n",
    "    for omic in LIST_OMICS:\n",
    "        GENE[omic] = dict_df_data['train'][omic].columns[\n",
    "#             dict_df_data['train'][omic].columns.str.upper().str.split(r'\\|').str[0].isin(top_genes)\n",
    "            dict_df_data['train'][omic].columns.str.upper().isin(top_genes)\n",
    "        ].to_numpy(copy=True).tolist()\n",
    "\n",
    "#         print(f'\\twith {omic} TOP {threshold}:', len(GENE[omic]))\n",
    "        ###\n",
    "        base_result[f'n_unq_{omic}'] = len(GENE[omic])\n",
    "        base_result[f'lst_unq_{omic}'] = GENE[omic]\n",
    "        ###\n",
    "    # NOTE THAT DNAmythyl and mRNA maybe have same genename in top gene => incresing num features comparing to num unique genes\n",
    "\n",
    "        for type_data in LIST_TYPE_DATA:\n",
    "            dict_df_data[type_data][omic] = dict_df_data[type_data][omic][GENE[omic]].copy(deep=True)\n",
    "    \n",
    "    dict_X = {}\n",
    "    dict_y = {}\n",
    "    for type_data in LIST_TYPE_DATA:\n",
    "        dict_X[type_data] = {}\n",
    "        dict_y[type_data] = {}\n",
    "\n",
    "    for type_omic in LIST_EXP_OMICS:\n",
    "        if '_' in type_omic:\n",
    "#             print(f'Creating data for multi-omics experiment: {type_omic}')\n",
    "            list_omics = type_omic.split('_')\n",
    "            for type_data in LIST_TYPE_DATA:\n",
    "                tuple_data_omics = tuple([dict_df_data[type_data][single_omic] for single_omic in list_omics])\n",
    "                dict_X[type_data][type_omic] = np.concatenate(tuple_data_omics, axis=1)\n",
    "        else:\n",
    "#             print(f'Creating data for single omic experiment: {type_omic}')\n",
    "            for type_data in LIST_TYPE_DATA:\n",
    "                dict_X[type_data][type_omic] = dict_df_data[type_data][type_omic].to_numpy(copy=True)\n",
    "\n",
    "        for type_data in LIST_TYPE_DATA:\n",
    "            dict_y[type_data][type_omic] = dict_df_label[type_data]['disease_subtypes'].to_numpy(copy=True)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    tmp_lst_dct_validate_biomarker_result = validate_biomarker(dict_X['train'], dict_y['train'], dict_X['test'], dict_y['test'],\n",
    "                       omics=LIST_EXP_OMICS, random_state=RANDOM_STATE,\n",
    "                       result_on_dataset= ['test'], rank_hparams_info =False,\n",
    "                       is_binary_problem = (len(LABEL_MAPPING_NAME)==2))\n",
    "    validate_result.extend([copy.deepcopy(base_result) for i in range(len(tmp_lst_dct_validate_biomarker_result))])\n",
    "    for dct_tmp, dct_val in zip(tmp_lst_dct_validate_biomarker_result, validate_result):\n",
    "        dct_val.update(dct_tmp)\n",
    "    return validate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.289962Z",
     "iopub.status.idle": "2025-08-28T08:32:11.290338Z",
     "shell.execute_reply": "2025-08-28T08:32:11.290171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# excluded_files = ['mogonet_full_top_biomarkers_sorted_desc_score_5models.csv']\n",
    "# excluded_files = [f'{BIOMARKERS_RESULT_FOLDER}/{COHORT}/'+biomarker_file for biomarker_file in excluded_files]\n",
    "\n",
    "excluded_files = [] # evaluation all candidate biomarkers result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.292438Z",
     "iopub.status.idle": "2025-08-28T08:32:11.292846Z",
     "shell.execute_reply": "2025-08-28T08:32:11.292640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Ignore ConvergenceWarning\n",
    "# warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "\n",
    "##\n",
    "result = []\n",
    "# THRESHOLD_LST = list(range(25, 401, 25))\n",
    "##\n",
    "\n",
    "for biomarker_file in list_loc_biomarkers:\n",
    "    if biomarker_file in excluded_files:\n",
    "        continue\n",
    "#     print(\"*\"*100)\n",
    "    baseline = biomarker_file.split('/')[-1].split('.')[-2]\n",
    "    printmd(baseline,'green')\n",
    "#         print(biomarker_file)\n",
    "\n",
    "    start = datetime.now()\n",
    "#     for threshold in THRESHOLD_LST:\n",
    "        ###\n",
    "    threshold = 'full'\n",
    "    tmp_result = []\n",
    "    base_init_dct_result = {'top': threshold, 'baseline': baseline}\n",
    "    tmp_validate_result = validate(biomarker_file,threshold)\n",
    "    tmp_result.extend([copy.deepcopy(base_init_dct_result) for i in range(len(tmp_validate_result))])\n",
    "    for dct_tmp, dct_val in zip(tmp_validate_result, tmp_result):\n",
    "        dct_val.update(dct_tmp)\n",
    "\n",
    "    result.extend(tmp_result)\n",
    "        \n",
    "    print(f'Total Time: {datetime.now()-start}')\n",
    "        ###\n",
    "#         print(f\"Top {threshold} - Using {tmp_result[0]['n_unq_markers']} uniques biomarkers in totals\")\n",
    "#         print(tmp_result[0]['lst_unq_markers'])\n",
    "#         print('\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.3. Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.293744Z",
     "iopub.status.idle": "2025-08-28T08:32:11.294170Z",
     "shell.execute_reply": "2025-08-28T08:32:11.293938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "avg_acc_all_models = {}\n",
    "avg_f1_all_models = {}\n",
    "filtered_results = []\n",
    "\n",
    "for res in result:\n",
    "#         print(res)\n",
    "    model_type = res['top']\n",
    "    baseline_type = res['model']\n",
    "    top_n = \"-\".join(res['baseline'].split('_')[:])\n",
    "    accuracy_t = res['test_acc']\n",
    "    f1_score_t = res.get('test_f1_macro', res.get('test_f1', 0))\n",
    "\n",
    "    filtered_results.append({\n",
    "        'model_type': model_type,\n",
    "        'baseline_type': baseline_type,\n",
    "        'top_n': top_n,\n",
    "        'accuracy_t': accuracy_t,\n",
    "        'f1_score_t': f1_score_t\n",
    "    })\n",
    "    \n",
    "    if model_type not in avg_acc_all_models:\n",
    "        avg_acc_all_models[model_type] = {}\n",
    "        avg_f1_all_models[model_type] = {}\n",
    "\n",
    "    if baseline_type not in avg_acc_all_models[model_type]:\n",
    "        avg_acc_all_models[model_type][baseline_type] = []\n",
    "        avg_f1_all_models[model_type][baseline_type] = []\n",
    "\n",
    "    avg_acc_all_models[model_type][baseline_type].append((top_n, accuracy_t))\n",
    "    avg_f1_all_models[model_type][baseline_type].append((top_n, f1_score_t))\n",
    "        \n",
    "for model_type in avg_acc_all_models:\n",
    "    for baseline_type in avg_acc_all_models[model_type]:\n",
    "        avg_acc_all_models[model_type][baseline_type].sort(key=lambda x: x[0])\n",
    "        avg_f1_all_models[model_type][baseline_type].sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.294981Z",
     "iopub.status.idle": "2025-08-28T08:32:11.295429Z",
     "shell.execute_reply": "2025-08-28T08:32:11.295232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, metric_name):\n",
    "    folder_save_fig = f\"/kaggle/working/cmp\"\n",
    "    if not os.path.exists(folder_save_fig):\n",
    "        os.makedirs(folder_save_fig)\n",
    "    path_save_fig = f'{folder_save_fig}/{metric_name}.png'\n",
    "    plt.figure(figsize = (8,4), dpi=300)\n",
    "    for baseline_name, values in metrics.items():\n",
    "        top_n_list = [item[0] for item in values]\n",
    "        metric_values = [item[1] for item in values]\n",
    "        plt.plot(top_n_list, metric_values, marker='o', label=baseline_name)\n",
    "    plt.title(f'{metric_name} for different baselines and top N features')\n",
    "    plt.xlabel('Top N features')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(fontsize='small', bbox_to_anchor=(1.05, 1), loc='upper left')    \n",
    "    plt.xticks(top_n_list)\n",
    "    plt.yticks(list(range(35, 96, 5)))\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_save_fig, facecolor='white', edgecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "# Plot accuracy\n",
    "for model_type in avg_acc_all_models.keys():\n",
    "    print('*' * 40, model_type, '*' * 40)\n",
    "    plot_metrics(avg_acc_all_models[model_type], f'Accuracy for {model_type}')\n",
    "    plot_metrics(avg_f1_all_models[model_type], f'F1 Score for {model_type}')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-28T08:32:11.296307Z",
     "iopub.status.idle": "2025-08-28T08:32:11.296672Z",
     "shell.execute_reply": "2025-08-28T08:32:11.296503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convenient for modifying plot\n",
    "json_file = \"/kaggle/working/results.json\"\n",
    "\n",
    "with open(json_file, mode='w', encoding='utf-8') as file:\n",
    "    json.dump(filtered_results, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5762188,
     "sourceId": 9474629,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5211395,
     "sourceId": 11454195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30096,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
